{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install cvlib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport os\nfrom numba import cuda\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport pywt\n\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\nimport cvlib as cv","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-12T16:35:35.841590Z","iopub.execute_input":"2022-05-12T16:35:35.841813Z","iopub.status.idle":"2022-05-12T16:35:41.349601Z","shell.execute_reply.started":"2022-05-12T16:35:35.841785Z","shell.execute_reply":"2022-05-12T16:35:41.348742Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# transforms tensor + normalization (0 mean, unit variance)\ntransform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0), (1))])\n\n# custom dataset loader     \nclass CustomDataset(Dataset):\n    def __init__(self, list_imgs, transform):\n        self.data = list_imgs\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        img_tensor = torch.zeros(len(self.data[idx])-1,self.data[idx][0].shape[0],self.data[idx][0].shape[1])\n        for i in range(len(self.data[idx])-1):\n            img_tensor[i,:,:] = self.transform(self.data[idx][i])\n            \n        class_id   = torch.tensor([self.data[idx][-1]])\n\n        return img_tensor.float(), class_id.float()","metadata":{"execution":{"iopub.status.busy":"2022-05-12T16:35:41.568039Z","iopub.execute_input":"2022-05-12T16:35:41.568258Z","iopub.status.idle":"2022-05-12T16:35:41.576728Z","shell.execute_reply.started":"2022-05-12T16:35:41.568231Z","shell.execute_reply":"2022-05-12T16:35:41.575884Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Reduced VGG16 model\nclass VGG16(nn.Module):\n    def __init__(self, in_channels):\n        super(VGG16, self).__init__()\n        self.conv1_1 = nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1)\n        self.conv2_1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.conv3_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.fc1 = nn.Linear(80000, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1_1(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv2_1(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv3_1(x))\n        x = self.maxpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, 0.5) # dropout to avoid overfitting\n        x = torch.sigmoid(self.fc2(x))\n        return x\n    \n\n# custom ultra-reduced VGG16 model   \nclass VGG16_reduced(nn.Module):\n    def __init__(self, in_channels):\n        super(VGG16_reduced, self).__init__()\n        self.conv1_1 = nn.Conv2d(in_channels=in_channels, out_channels=10, kernel_size=3, padding=1)\n\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.fc1 = nn.Linear(100000, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1_1(x))\n        x = self.maxpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, 0.5) # dropout to avoid overfitting\n        x = torch.sigmoid(self.fc2(x))\n        return x\n    \ndef train(first_epoch, num_epochs, model, criterion, optimizer, train_loader, valid_loader, all_targets):\n\n    train_losses, valid_losses = [],[]\n\n    for epoch in range(first_epoch, first_epoch + num_epochs):\n\n        # training phase\n        train_loss, train_acc = train_for_epoch(model, criterion, optimizer, train_loader)\n\n        # validation phase\n        valid_loss, valid_acc = validate(model, criterion, valid_loader, all_targets)        \n\n        print(f'[{epoch:03d}] train loss: {train_loss:04f}  '\n              f'val loss: {valid_loss:04f}  '\n              f'train acc: {train_acc*100:.4f}  '\n              f'val acc: {valid_acc*100:.4f}%')\n\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n        \n    torch.save(model.state_dict(), '\\kaggle\\working\\df_model'+str(epoch))        \n\n\ndef train_for_epoch(model, criterion, optimizer, train_loader):\n\n    # put model in train mode\n    model.train()\n\n    # keep track of the training losses during the epoch\n    train_losses = []\n    y_pred = []\n    y_true = []\n\n    for batch, targets in train_loader:\n        # Move the training data to the GPU\n        batch = batch.to(device)\n        targets = targets.to(device)\n\n        # clear previous gradient computation\n        optimizer.zero_grad()\n\n        # forward propagation\n        predictions = model(batch)\n\n        # calculate the loss\n        loss = criterion(predictions, targets)\n\n        # backpropagate to compute gradients\n        loss.backward()\n\n        # update model weights\n        optimizer.step()\n\n        # update average loss\n        train_losses.append(loss.item())\n        \n        # save predictions\n        y_pred.extend(predictions.cpu().detach().numpy().round())\n        y_true.extend(targets.cpu().detach().numpy())\n\n    # calculate average training loss\n    train_loss = np.sum(train_losses)/len(train_losses)\n    \n    # Collect predictions into y_pred and ground truth into y_true\n    y_pred = np.array(y_pred, dtype=np.float32)\n    y_true = np.array(y_true, dtype=np.float32)\n    \n    y_pred = np.squeeze(y_pred)\n    y_true = np.squeeze(y_true)\n\n    # Calculate accuracy as the average number of times y_true == y_pred\n    accuracy = np.sum(y_pred==y_true)/len(y_pred)\n\n    return train_loss, accuracy\n\n\ndef validate(model, criterion, valid_loader, all_targets):\n\n    # put model in evaluation mode\n    model.eval()\n\n    # keep track of losses and predictions\n    valid_losses = []\n    y_pred = []\n\n    # We don't need gradients for validation, so wrap in \n    # no_grad to save memory\n    with torch.no_grad():\n\n        for batch, targets in valid_loader:\n\n            # Move the training batch to the GPU\n            batch = batch.to(device)\n            targets = targets.to(device)\n\n            # forward propagation\n            predictions = model(batch)\n\n            # calculate the loss\n            # loss = criterion(predictions, targets.squeeze())\n            loss = criterion(predictions, targets)\n\n\n            # update running loss value\n            valid_losses.append(loss.item())\n\n            # save predictions\n#             y_pred.extend(predictions.argmax(dim=1).cpu().numpy())\n            y_pred.extend(predictions.cpu().numpy().round())\n\n\n    # compute the average validation loss\n    valid_loss = np.mean(valid_losses)\n\n    # Collect predictions into y_pred and ground truth into y_true\n    y_pred = np.array(y_pred, dtype=np.float32)\n    y_true = np.array(all_targets, dtype=np.float32)\n    \n    y_pred = np.squeeze(y_pred)\n\n    # Calculate accuracy as the average number of times y_true == y_pred\n    accuracy = np.sum(y_pred==y_true)/len(y_pred)\n\n    return valid_loss, accuracy","metadata":{"execution":{"iopub.status.busy":"2022-05-12T16:35:42.717203Z","iopub.execute_input":"2022-05-12T16:35:42.717475Z","iopub.status.idle":"2022-05-12T16:35:42.744404Z","shell.execute_reply.started":"2022-05-12T16:35:42.717444Z","shell.execute_reply":"2022-05-12T16:35:42.743728Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# function to load the dataset\ndef load_dataset(folder_path, type_face):\n\n    images = []\n\n    for number_folder in os.listdir(folder_path + type_face):\n\n        for image_name in os.listdir(folder_path + type_face + number_folder + '/'):\n            img = cv2.imread(folder_path + type_face + number_folder + '/' + image_name, 1)\n            images.append(img)\n\n    return images\n\n# function to plot one image\ndef plot_single_img(img):\n    rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(rgb)\n    plt.show()\n\n# function that finds a returns the cropped face from the input image\ndef face_detector_cvlib(img, mode, resize=False, x=150, y=220):\n    \n    faces, _ = cv.detect_face(img)\n    \n    try:\n        \n        # Try get the first face prediction\n        x1, y1, x2, y2 = faces[0]\n\n        # Check other predictions if some of these statements doesn't hold\n        if (x1<0 or y1<0 or x2>img.shape[1] or y2>img.shape[0]):\n            for i in range(1, len(faces)):\n                x1, y1, x2, y2 = faces[i]\n                # break when prediction is within the boundaries of the img\n                if (x1>0 and y1>0 and x2<img.shape[1] and y2<img.shape[0]):\n                    break\n        \n        if not resize:\n            # return cropped img\n            return img[y1:y2, x1:x2]\n        else:\n            return cv2.resize(img[y1:y2, x1:x2], (x, y))\n                \n    except:\n        \n        print('No face was detected')\n\n        if mode==\"development\":\n            pass\n        else:\n            return cv2.resize(img, (x, y))\n\n# function to create the gabor filters \ndef get_gabor_filters(k_size=31, n_theta=16, lambda_val=2, sigma_val=4, gamma_val=0.5, psi_val=0):\n\n    # theta  = orientation\n    # sigma  = standard deviation of the gaussian envelope\n    # lambda = wavelength of the sinusoidal factor.\n    # gamma  = spatial aspect ratio\n    # phi    = pahse offset\n\n    filters = []\n    for psi_value in psi_val:\n        for sigma_value in sigma_val:\n            for gamma_value in gamma_val:\n                for lambda_value in lambda_val:\n                    for theta_value in np.arange(0, np.pi, np.pi / n_theta):\n                        kern = cv2.getGaborKernel(ksize=(k_size, k_size), sigma=sigma_value, theta=theta_value, lambd=lambda_value, gamma=gamma_value, psi=psi_value, ktype=cv2.CV_32F)\n                        # kern /= 1.5*kern.sum()\n                        # kern /= kern.sum()\n                        filters.append(kern)\n\n    return filters\n\n# function to filter the images with the gabor filters\ndef gabor_filter(img, filters):\n    filtered_img = np.zeros_like(img)\n    for kern in filters:\n        fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n        np.maximum(filtered_img, fimg, filtered_img)\n\n    return filtered_img\n\n# function to normalize the images within a range (max_value)\ndef normalize_img(img, max_value):\n\n    max_f = np.max(img)\n    min_f = np.min(img)\n\n    img = np.round(((img - min_f) / (max_f - min_f)) * max_value)\n    \n    return img.astype('int32')\n    \n# function that returns the coocurrence matrix of an image\ndef co_ocurrence_matrix(gray):\n\n    coom = np.zeros(shape=([4, 200, 200]))\n    \n    gray_norm = normalize_img(gray, 199)\n\n\n    for i in range(gray_norm.shape[0]):\n        for j in range(gray_norm.shape[1]):\n            \n            #   0 degrees\n            if j != gray_norm.shape[1] - 1:\n                coom[0, gray_norm[i,j], gray_norm[i,j+1]] += 1\n            #  90 degrees\n            if i != gray_norm.shape[0] - 1:\n                coom[2, gray_norm[i,j], gray_norm[i+1,j]] += 1\n\n            if i < gray_norm.shape[0] - 1 and j < gray_norm.shape[1] - 1:\n                #  45 degrees\n                coom[1, gray_norm[i+1,j], gray_norm[i,j+1]] += 1\n                # 135 degrees\n                coom[3, gray_norm[i,j], gray_norm[i+1,j+1]] += 1\n\n    return coom\n\n# function that returns the wavelet transform at different levels of an image \ndef wavelet_tranform(img, levels=5, w_type = 'haar', color=False): #9 features\n\n    feature_list = []\n\n    if color:\n\n        caR = img[:,:,2]\n        caG = img[:,:,1]\n        caB = img[:,:,0]\n\n        # wavelet transform for number of levels\n        for i in range(levels):\n\n            caR, (chR, cvR, cdR) = pywt.dwt2(caR, wavelet=w_type)\n            caG, (chG, cvG, cdG) = pywt.dwt2(caG, wavelet=w_type)\n            caB, (chB, cvB, cdB) = pywt.dwt2(caB, wavelet=w_type)\n\n            ch = np.stack([chR,chG,chB], axis=2)\n            cv = np.stack([cvR,cvG,cvB], axis=2)\n            cd = np.stack([cdR,cdG,cdB], axis=2)\n            print(ch.shape)\n            feature_list.extend([chR,cvR,cdR,chG,cvG,cdG,chB,cvB,cdB])\n            \n        ca = np.stack([caR,caG,caB], axis=2)\n        feature_list.extend([caR,caG,caB])\n\n\n\n    # gray level\n    else:\n\n        # compute the features from the different levels\n        for i in range(levels):\n\n            ca, (ch, cv, cd) = pywt.dwt2(img, wavelet=w_type)\n            img = ca\n\n            # standard deviation\n            feature_list.append(ch)\n            feature_list.append(cv)\n            feature_list.append(cd)\n    \n        feature_list.append(ca)\n\n    return  feature_list\n\n\n# kernel definition to gradient extraction (horizontal and vertical edges)\nkernelx = np.array([[ 1,  2,  1],\n                    [ 0,  1,  0],\n                    [-1, -2, -1]])\n\nkernely = np.array([[-1,  0,  1],\n                    [-2,  0,  2],\n                    [-1,  0,  1]])\n\n# function that returns the image filtered and the magnitude of the gradient\ndef compute_gradient(gray): #5 features\n\n    grad_list = []\n    Gx   = cv2.filter2D(gray, ddepth=cv2.CV_32F, kernel=kernelx) # enough with CV_32F?\n    Gy   = cv2.filter2D(gray, ddepth=cv2.CV_32F, kernel=kernely)\n    magnitude = np.sqrt(Gx**2 + Gy**2)\n    grad_list.extend([Gx, Gy, magnitude])\n    \n    return grad_list \n\ndef compute_AUC(y_true, y_pred_prob, save_plot = False):\n    \n    # transpose so 1 is the positive label\n    y_true = 1 - y_true\n    # therefore, invert probabilities as well\n    y_pred_prob = 1. - y_pred_prob\n\n    # compute the roc curve\n    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred_prob, pos_label=1)\n    # compute the AUC\n    auc_val = metrics.auc(fpr, tpr)\n    \n    print('AUC: ',auc_val)\n    fig = plt.figure(figsize=(12,8))\n    plt.plot(fpr, tpr, 'g')\n    plt.title('AUC = '+str(\"{:.3f}\".format(auc_val)))\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.figure(tight_layout=True)\n    fig.savefig('AUC_'+str(\"{:.3f}\".format(auc_val))+'.jpg')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T16:35:44.770911Z","iopub.execute_input":"2022-05-12T16:35:44.771424Z","iopub.status.idle":"2022-05-12T16:35:44.807802Z","shell.execute_reply.started":"2022-05-12T16:35:44.771391Z","shell.execute_reply":"2022-05-12T16:35:44.807048Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"## PARAMETERS\ntask_folder = [\"../input/task1-df/Task_1/\",\"../input/task2df/Task_2_3/\"]\nfolder_path = [\"development\", \"evaluation\"]\noptions = ['real/', 'fake/']\nwidth_img  = 200\nheight_img = 200\n\n# split in training and validation \nP_SPLIT = True\n\n# TASK SELECTION\n# 1 = TASK 1\n# 2 = TASK 2\nF_TASK = 1\n\n\ngf_theta  = 16\ngf_lambda = [1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]\ngf_sigma  = [4]\n# gf_phi    = [0, 0.25, 0.41] # 0\ngf_phi    = [0] # 0\n# gf_gamma  = [0.35, 0.5, 0.75] # 0.5\ngf_gamma  = [0.5] # 0.5\n\n\n\n# process\nF_EXTRACT_FEATURES = True\n\nif F_EXTRACT_FEATURES:\n    for n_main in range(2):\n\n        # 1. LOAD DATASET\n        print('1. Loading dataset...')\n        real_faces = load_dataset(folder_path = (task_folder[n_main*(F_TASK-1)] + folder_path[n_main] + \"/\"), type_face = options[0])\n        fake_faces = load_dataset(folder_path = (task_folder[n_main*(F_TASK-1)] + folder_path[n_main] + \"/\"), type_face = options[1])\n\n\n        # 2. CROP THE FACE REGION\n        print('2. Cropping faces')\n        real_faces_cropped = []\n        fake_faces_cropped = []\n\n\n        for i in range(len(real_faces)):\n            img = face_detector_cvlib(real_faces[i], folder_path[n_main], resize=True, x=width_img, y=height_img)\n            if isinstance(img, np.ndarray):\n                real_faces_cropped.append(img)\n\n        for i in range(len(fake_faces)):\n            img = face_detector_cvlib(fake_faces[i], folder_path[n_main], resize=True, x=width_img, y=height_img)\n            if isinstance(img, np.ndarray):\n                fake_faces_cropped.append(img)\n\n        real_feat = []\n        fake_feat = []\n\n        # 3. GET GABOR FILTERS (append labels)\n        print('3. Creating filters')\n        gabor_filters = get_gabor_filters(k_size=31, n_theta=gf_theta, lambda_val=gf_lambda, sigma_val=gf_sigma, gamma_val=gf_gamma, psi_val=gf_phi)\n        \n        # 4. FILTER IMAGES\n        print('4. Filtering images')\n        # 4.1 Real faces\n        for i, rgb_img in enumerate(real_faces_cropped):\n            img_gray = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n            \n            # GABOR FILTERS\n            imgs_feat = []\n            for f in range(0, len(gabor_filters), gf_theta):\n                imgs_feat.append(gabor_filter(img = img_gray, filters = gabor_filters[f:f+gf_theta]))\n                \n            # CO-OCCURENCE MATRIX\n#             imgs_feat.extend(co_ocurrence_matrix(img_gray))\n\n            # WAVELET (can not append, different sizes (levels))\n#             imgs_feat = imgs_feat + wavelet_tranform(img, levels=2, w_type = 'haar', color=True)\n            \n            # GRAD\n#             imgs_feat = imgs_feat + compute_gradient(img_gray)\n            \n            # RGB\n            imgs_feat.extend([rgb_img[:,:,0],rgb_img[:,:,1],rgb_img[:,:,2], 0]) # 0 REAL\n            \n            # FINAL FEATURES\n            real_feat.append(imgs_feat)\n            \n        # 4.2 Fake faces\n        for i, rgb_img in enumerate(fake_faces_cropped):\n            img_gray = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2GRAY)\n            \n            # GABOR FILTERS\n            imgs_feat = []\n            for f in range(0, len(gabor_filters), gf_theta):\n                imgs_feat.append(gabor_filter(img = img_gray, filters = gabor_filters[f:f+gf_theta]))\n                \n            # CO-OCCURENCE MATRIX\n#             imgs_feat.extend(co_ocurrence_matrix(img_gray))\n            \n            # WAVELET (can not append, different sizes (levels))\n#             imgs_feat = imgs_feat + wavelet_tranform(img, levels=2, w_type = 'haar', color=True)\n\n            # GRAD\n#             imgs_feat = imgs_feat + compute_gradient(img_gray)\n            \n            # RGB\n            imgs_feat.extend([rgb_img[:,:,0],rgb_img[:,:,1],rgb_img[:,:,2], 1]) # 1 FAKE\n            \n            # FINAL FEATURES\n            fake_feat.append(imgs_feat)\n\n        # 5. SPLIT TRAIN AND TEST\n        print('4. Splitting data')\n        if n_main==0:\n            \n            if P_SPLIT:\n                n_split = 300\n                train_imgs = real_feat[:n_split] + fake_feat[:n_split]\n\n                val_imgs   = real_feat[n_split:] + fake_feat[n_split:]\n                dataset_val   = CustomDataset(list_imgs=val_imgs, transform=transform)\n                len_real_val = len(real_feat[n_split:])\n                \n            else:\n                train_imgs = real_feat + fake_feat\n            \n            dataset_train = CustomDataset(list_imgs=train_imgs, transform=transform)\n\n        else:\n            test_imgs = real_feat + fake_feat\n            dataset_test = CustomDataset(list_imgs=test_imgs, transform=transform)\n            \n            if P_SPLIT:\n                all_targets = np.ones([len(val_imgs)])\n                all_targets[:len_real_val] = 0\n            else:\n                all_targets = np.ones([len(test_imgs)])\n                all_targets[:len(real_feat)] = 0\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Number of channels: ', train_imgs[0][0].shape)\n\n# 5. CREATE LOADERS\ntrain_loader = DataLoader(dataset_train, batch_size=64, num_workers=0, shuffle=True)\ntest_loader  = DataLoader(dataset_test, batch_size=10, num_workers=0)  \n\nif P_SPLIT:\n    val_loader = DataLoader(dataset_val, batch_size=10, num_workers=0)\n\n\n# 6. INIT MODEL\n# model = VGG16(in_channels = 15)\nmodel = VGG16_reduced(in_channels = 12)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# 7. LOSS AND OPTIMIZER\ncriterion = nn.BCELoss()\n# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n\n# 8. TRAIN\nprint('8. Training model')\n\nif P_SPLIT:\n    # training to choose the architecture and paramiters of the model\n    train(1, 13, model, criterion, optimizer, train_loader, val_loader, all_targets)\nelse:\n    # last training with all the images\n    train(1, 13, model, criterion, optimizer, train_loader, test_loader, all_targets)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load same model but without the dropout layer (added to figth against overfitting through training) so we get the same output every time\nclass VGG16_reduced(nn.Module):\n    def __init__(self, in_channels):\n        super(VGG16_reduced, self).__init__()\n        self.conv1_1 = nn.Conv2d(in_channels=in_channels, out_channels=10, kernel_size=3, padding=1)\n\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        self.fc1 = nn.Linear(100000, 128)\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1_1(x))\n        x = self.maxpool(x)\n        x = x.reshape(x.shape[0], -1)\n        x = F.relu(self.fc1(x))\n        x = torch.sigmoid(self.fc2(x))\n        return x\n\nmodel = VGG16_reduced(in_channels = 12)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the weigths of the last epoch \n# model.load_state_dict(torch.load('\\kaggle\\working\\df_model13'))\nmodel.load_state_dict(torch.load('../input/weigths-tr/_kaggle_working_df_model13'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EVALUATE THE MODEL\n\nall_targets_test = np.ones([len(test_imgs)])\nall_targets_test[:len(real_feat)] = 0\n\n# put model in evaluation mode\nmodel.eval()\n\ny_pred = []\ny_pred_prob = []\n\n# We don't need gradients for validation, so wrap in \n# no_grad to save memory\nwith torch.no_grad():\n\n    for batch, targets in test_loader:\n\n        # Move the training batch to the GPU\n        batch = batch.to(device)\n        targets = targets.to(device)\n\n        # forward propagation\n        predictions = model(batch)\n\n        # save predictions\n        y_pred_prob.extend(predictions.cpu().numpy())\n        y_pred.extend(predictions.cpu().numpy().round())\n\n\n\n# Collect predictions into y_pred and ground truth into y_true\ny_pred = np.array(y_pred, dtype=np.float32)\ny_pred_prob = np.array(y_pred_prob, dtype=np.float32)\n\ny_true = np.array(all_targets_test, dtype=np.float32)\n\ny_pred = np.squeeze(y_pred)\ny_pred_prob = np.squeeze(y_pred_prob)\n\n\n# Calculate accuracy as the average number of times y_true == y_pred\naccuracy = np.sum(y_pred==y_true)/len(y_pred)\n\n# print confusion matrix\nCM = confusion_matrix(y_true, y_pred)\nprint(CM)\n\nprint(accuracy)\nprint(y_pred)\nprint(y_pred_prob)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute the AUC and plot\ncompute_AUC(y_true, y_pred_prob, save_plot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the labels, predictions and probabilities\nnp.save('y_true',y_true)\nnp.save('y_pred',y_pred)\nnp.save('y_pred_prob',y_pred_prob)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T14:56:54.865937Z","iopub.execute_input":"2022-05-09T14:56:54.866707Z","iopub.status.idle":"2022-05-09T14:56:54.872714Z","shell.execute_reply.started":"2022-05-09T14:56:54.866651Z","shell.execute_reply":"2022-05-09T14:56:54.871921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# See the results\nlist_rf = ['Real', 'Fake']\ntest_images = real_faces + fake_faces\nfor idx, (pp, gt) in enumerate(zip(y_pred, y_true)):\n    print('GroundTruth: ',list_rf[int(gt)],' Prediction: ',list_rf[int(pp)])\n    plot_single_img(test_images[idx])\n    input()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}